\chapter{Numerical Optimisation}
\localtableofcontents

\section{Wolfe Conditions}

Wolfe conditions relate to inexact line search methods i.e.
where you solve a subproblem on the form:
\[
\min_\alpha f(x_k + \alpha p_k),
\]
where $p_k$ is a search direction. The inexact line searches provide
an efficient way of computing an acceptable step length $\alpha$.
The Wolfe conditions are a set of inequalities that tell us whether
the chosen step size:
\begin{enumerate}
    \item Still provides a descent direction,
    \item the descent direction has reduced the slope sufficiently.
\end{enumerate}

\section{Interior Point Methods}

Interior point methods are widely used for nonlinear programs.\\

Interior point methods approximates the constraints of a linear
programming model as a set of boundaries surrounding a region.
These approximations are used when the problem has constraints
that are discontinuous or otherwise troublesome, but can me
modified so that a linear solver can handle them. 

Two practical algorithms exist in IP method: barrier and
primal-dual.

\subsection{Barrier Approach}

This refers to the reformulation of an NLP to a Lagrangian,
and to fix the "barriers" i.e. penalty on the Lagrange terms 
being non-zero and solve for the primal variable. The common
example is a $\log$ barrier.

We then derive the KKT condition. Newton's
method is used to iteratively approach more and more optimal
solutions within the feasible region.\\

\subsection{Primal-dual Method}
Primal-dual method is a more promising way to solve
larger problems with more efficiency and accuracy.


\section{Trust Region Method}

A trust region is the subset of the region of the objective function that
is approximated using a model function e.g. a quadratic function.


\section{Newton Method}

Suppose we want to solve:
\[
\min_x f(x).
\]
for $x\mathcal{R}^n$. Newton's method uses the Taylor expansion:
\[
f(x+\delta x) \approx f(x) + f'(x)\delta x + \half\delta x\transp H\delta x,
\]
to find the optimal step:
\begin{equation}\label{eq:newton_step}
\delta x^* = - H\inv f'(x).
\end{equation}

\section{Gauss-Newton}

When we solve a \emph{least-squares} optimisation problem on the form:
\[
\min_x = \sum_i f_i^2(x_i),
\]
the Hessian  is given by:
\[
H := \sum_i \nabla f_i \nabla f_j\transp  + f_i \Delta f_j.
\]
A Gauss-Newton step uses the following approximation:
\[
H_{GN} := \sum_i \nabla f_i \nabla f_j\transp,
\]
resulting in the following update step:
\begin{equation}\label{eq:newton_step:gauss-newton}
\delta x^* = - H_{GN}\inv \sum_i \nabla f_i(x).
\end{equation}

\section{Levenberg-Marquardt}

Uses the following Newton step rather than \eqref{eq:newton_step:gauss-newton}:
\begin{equation}\label{eq:newton_step:lm}
\delta x^* = - (H_{GN} + \lambda I)\inv \nabla f(x),
\end{equation}
for some $\lambda>0$, where $H_{GN}$ is the Gauss-Newton Hessian approximation.

\section{Quasi-Newton}

\section{Broyden–Fletcher–Goldfarb–Shanno}

\section{Stochastic Gradient Descent}

We consider a problem on the form:
\[  
\min_x \frac1n \sum_i f_i(x).
\]
Usually each $f_i$ corresponds to some operation on the $i$th part of
a data set. Standard gradient descent leads to steps on the form:
\[
x^{k+1} = x^k - \frac1n \sum_i \nabla f_i (x^k).
\]
The \emph{stochastic} gradient descent simply performs the update on a
\emph{per training sample} basis:
\[
x^{k+1} = x^k - \nabla f_i (x^k),
\]
for all $i$. A compromise between this and the true gradient is the
so-called \emph{mini-batch strategy}:

\[
x^{k+1} = x^k - \frac{1}{|\sigma|}\sum_{\sigma}\nabla f_i (x^k),
\]
where $\sigma$ is a subset of the training samples.

\section{ADMM}

Consider:
\begin{align}
& \min_{x,y} f(x) + g(z)\\
& s.t.\quad Ax + Bz = b.
\end{align}

The augmented Langrangian reads:
\[
L_\rho(x,z,y) =  f(x) + g(z) + y\transp (Ax + Bz - b) + \rho/2\|Ax + Bz - b\|.
\]
ADMM solves this by iterating:
\begin{align}
& x^{k+1} = \arg\min_x L_\rho (x, z^k, y^k),\\
& z^{k+1} = \arg\min_z L_\rho (x^k, z, y^k),\\
& y^{k+1} = y^k + \rho(Ax^{k+1} + Bz^{k+1} - b).
\end{align}

\emph{Dual descent on the augmented Lagrangian is called the
method of multipliers!}


Inexact ADMM solves the subproblems using CG. Converges when the
error is summable, whatever that means. However, not great when
the condition number is awful. Inspired by linearised ADMM, the
sub (optimisation) problem for the $x$ iterate replaced by a second
order Taylor expansion. They then solve for its stationary point.

NysADMM preconditions the CG method applied to the system above
with a randomly sketched Nystrom approximation of the Hessian.

\section{Scalable Optimisation Methods}

\section{Robust Optimisation}

\section{Distributionally Robust Optimisation}

% Choice of Ambiguity Set of Probability Distributions

% Connection with second-order cone programming or semidefinite programs

    % 205 takes a step forward and formulates the problem as a CONVEX problem
    
     Bansal et al. [9] stud 

\section{PDE-constrained Optimisation}


\subsection{Mesh-dependence}

\input{geometric}